# Changelog

All notable changes to this project will be documented in this file.

## [1.5.1] - 2026-02-18

### Bug fixes

- Fixed `--skip-existing` in multi-format mode (`--format srt,text`) â€” previously only checked for the first format's output file; now skips only when ALL requested format outputs already exist
- Fixed `--no-timestamps` conflict check missing `lrc`, `ass`, and `ttml` formats â€” those formats all require timing info and are now correctly listed as incompatible
- Fixed `--speaker-names` silently doing nothing when `--diarize` is not set â€” now prints a clear warning

### UX improvements

- Batch summary now shows skipped file count when `--skip-existing` is active: `ðŸ“Š Done: 8 files (3 skipped), ...`

## [1.5.0] - 2026-02-18

### Bug fixes

- Fixed `--clean-filler` not stripping filler words from the word list â€” subtitle formatters (`--max-chars-per-line` / `--max-words-per-line`) use the word list directly, so filler words reappeared in SRT/VTT/ASS/TTML output
- Fixed orphaned punctuation after filler removal (double commas, leading commas, `,?` â†’ `?`)
- Fixed `--search-fuzzy` always failing on short queries â€” now matches per word token (similarity â‰¥0.6) instead of comparing the short query against full segment text
- Fixed URL + `--detect-language-only` leaving temp directories behind; also changed errors to continue remaining files instead of aborting the batch
- Fixed `setup.sh --check` hanging 60+ seconds when pyannote.audio is installed â€” now uses lightweight metadata lookup instead of a full import; completes in ~12s
- Fixed `skill.json`: moved `ffmpeg` from `requires.bins` to `optionalBins` â€” only needed for `--burn-in`, `--normalize`, `--denoise`, `--channel`, `--export-speakers`

### Subtitle formats

- **`--format ass`** â€” Advanced SubStation Alpha subtitles (Aegisub, mpv, VLC, MPC-HC); supports `--max-words-per-line` and diarization labels
- **`--format lrc`** â€” Timed lyrics format for music players (foobar2000, AIMP, VLC)
- **`--format html`** â€” Confidence-colored HTML transcript: green (â‰¥0.8), yellow (â‰¥0.5), red (<0.5); requires `--word-timestamps`
- **`--format ttml`** â€” W3C TTML 1.0 / DFXP broadcast subtitles (Netflix, Amazon Prime, BBC)
- **`--format csv`** â€” RFC 4180 CSV with header row; speaker column auto-added when `--diarize` is set

### Transcript tools

- **`--search TERM`** â€” Find all timestamps where a word/phrase appears; replaces normal transcript output
- **`--search-fuzzy`** â€” Approximate matching with `--search` (similarity â‰¥0.6 per token); finds "wrld" â†’ "world"
- **`--detect-chapters`** â€” Auto-detect chapter breaks from silence gaps; outputs YouTube-style timestamps
- **`--chapter-gap SEC`** â€” Minimum silence for a chapter break (default: 8s); lower for dense content
- **`--chapters-file PATH`** â€” Save chapters to a file so they don't mix into the transcript; avoid in batch mode (single path gets overwritten per file)
- **`--chapter-format youtube|text|json`** â€” Format for chapter output; `youtube` (default) is paste-ready for YouTube descriptions; with `--format json`, chapters embed under a `"chapters"` key
- **`--export-speakers DIR`** â€” Post-diarization, save each speaker's audio turns as separate WAV files; requires `--diarize` and ffmpeg

### Batch improvements

- **Batch ETA** (no flag) â€” Automatically shows `[N/total] filename | ETA: Xm Ys` for sequential batch jobs
- **`--language-map "pat=lang,..."`** â€” Per-file language override in batch using fnmatch globs; also accepts `@file.json`
- **`--retries N`** â€” Retry failed files with exponential backoff (2s, 4s, 8sâ€¦); shows failed-file summary at end; ignored in `--parallel` mode
- **`--rss URL`** â€” Download and transcribe podcast episodes from an RSS feed; always pair with `-o <dir>`
- **`--rss-latest N`** â€” Number of episodes to fetch from RSS (default: 5; 0 = all)
- **`--skip-existing`** â€” Skip files that already have output; safe for resuming interrupted batches
- **`--parallel N`** â€” Process N files simultaneously; mainly useful for CPU batch jobs (GPU handles one file efficiently on its own)
- **`--output-template`** â€” Custom filename pattern for batch output (e.g., `"{stem}_{lang}.{ext}"`)
- **`--stats-file PATH`** â€” Write performance stats JSON (duration, processing time, RTF) after transcription; pass a dir in batch mode for one file per input
- **`--merge-sentences`** â€” Merge short fragment segments into sentence-length chunks before writing

### Model & inference

- **Default model â†’ `distil-large-v3.5`** â€” Better accuracy (7.08 vs 7.53 WER), same speed; trained on 4Ã— more data
- **Auto-disable `condition_on_previous_text` for distil-* models** â€” Prevents repetition loops; HuggingFace recommendation; applied automatically, no flag needed
- **`--condition-on-previous-text`** â€” Override to re-enable context conditioning (off by default for distil models)
- **`--log-level debug|info|warning|error`** â€” Control faster-whisper library verbosity; `debug` for diagnosing issues, `error` for clean pipeline output
- **`--model-dir PATH`** â€” Custom HuggingFace model cache dir; also works with locally converted CTranslate2 models
- **`--no-timestamps`** â€” Output plain text only, no timestamps
- **`--chunk-length SEC`** â€” Audio chunk size for batched inference; smaller = less memory, may affect accuracy at boundaries
- **`--length-penalty FLOAT`** â€” Bias beam search toward shorter (<1.0) or longer (>1.0) outputs
- **`--repetition-penalty FLOAT`** â€” Penalize repeated phrases; values >1.0 reduce looping
- **`--no-repeat-ngram-size N`** â€” Hard-block any N-gram from appearing twice in output
- **`--clip-timestamps "START,END"`** â€” Transcribe only between START and END seconds (e.g., `"30,60"`)
- **`--stream`** â€” Print segments progressively as transcribed instead of waiting for the full file
- **`--progress`** â€” Show a progress bar during transcription
- **`--best-of N`** â€” Sample N candidates in greedy mode and pick the best; higher = more accurate but slower
- **`--patience FLOAT`** â€” Beam search patience multiplier; >1.0 explores more hypotheses at cost of speed
- **`--max-new-tokens N`** â€” Max tokens generated per audio chunk; limits runaway generation on silence/music
- **`--hotwords WORDS`** â€” Boost specific words in the decoder; unlike `--initial-prompt`, directly biases token probabilities; best for rare terms that keep getting missed
- **`--prefix TEXT`** â€” Seed the first segment with known opening text; only affects the first chunk
- **`--revision VERSION`** â€” Pin a specific HuggingFace model revision for reproducible transcription
- **`--suppress-tokens`** â€” Comma-separated token IDs to block during decoding
- **`--max-initial-timestamp FLOAT`** â€” Cap the timestamp of the first token; prevents long leading silences from being misread
- **`--vad-threshold FLOAT`** (default: 0.5) â€” Speech detection sensitivity; raise to 0.6â€“0.7 if background noise is being transcribed
- **`--vad-neg-threshold FLOAT`** â€” Hysteresis threshold; speech ends only when probability drops below this
- **`--min-speech-duration-ms MS`** â€” Minimum segment length to count as speech; raise to filter short noise bursts
- **`--max-speech-duration-s SEC`** â€” Maximum segment length; longer segments are split at this boundary
- **`--min-silence-duration-ms MS`** â€” Minimum gap between segments; increase to join segments split by brief pauses
- **`--speech-pad-ms MS`** â€” Padding around detected speech; increase to avoid clipping word edges
- **`--temperature FLOAT`** â€” Decoding temperature; 0.0 = greedy/deterministic (recommended); higher adds randomness
- **`--no-speech-threshold FLOAT`** â€” Drop segments where no-speech probability exceeds this; raise to 0.8 to reject music/silence hallucinations
- **`--hallucination-silence-threshold FLOAT`** â€” Drop segments where Whisper generates text over near-silence; helps with ambient music or background noise

### Speaker & quality

- **`--speaker-names "Alice,Bob"`** â€” Replace `SPEAKER_1`/`SPEAKER_2` with real names in order; requires `--diarize`; also renames WAV files from `--export-speakers`
- **`--filter-hallucinations`** â€” Remove sound markers (`[Music]`, `[Applause]`), common outros, and exact-duplicate consecutive segments
- **`--burn-in OUTPUT`** â€” Burn subtitles into a video as a hard-coded overlay via ffmpeg; requires a video input and ffmpeg
- **`--keep-temp`** â€” Keep URL-downloaded audio after transcription; avoids re-downloading if you re-process the same URL
- **`--min-speakers N` / `--max-speakers N`** â€” Hint pyannote about expected speaker count; improves diarization when you already know it (e.g., `--min-speakers 2 --max-speakers 2` for a 1-on-1)

### Audio preprocessing

- **`--clean-filler`** â€” Strip `um`, `uh`, `er`, `ah`, `hmm`, `you know`, `I mean`, `you see` from output; filters both segment text and word list so subtitles also omit them
- **`--channel left|right|mix`** â€” Extract one stereo channel before transcribing (default: `mix`); useful for dual-speaker interviews where each person is on one channel
- **`--max-chars-per-line N`** â€” Character-based subtitle wrapping (e.g., 42 for Netflix style); takes priority over `--max-words-per-line`
- **`--detect-paragraphs`** â€” Insert `\n\n` breaks in text output at natural pause/sentence boundaries
- **`--paragraph-gap SEC`** â€” Gap threshold for paragraph detection (default: 3.0s); lower for fast-paced speech

### Setup & agent compatibility

- **`setup.sh --check`** â€” System diagnostic: GPU/CUDA, Python, ffmpeg, venv, faster-whisper, pyannote.audio, yt-dlp, HuggingFace token
- **ffmpeg now optional** â€” PyAV handles basic decoding; ffmpeg only needed for `--burn-in`, `--normalize`, `--denoise`, `--channel`, `--export-speakers`
- **Chapter stdout uses `=== CHAPTERS (N) ===` separator** â€” Makes it easy for scripts/agents to split chapter data from transcript text
- **`--quiet` / `-q` now suppresses the RTX 3070 compute-type tip**
- **`--format json --detect-chapters`** â€” Chapters now embedded under `"chapters"` key in JSON output

---

## [1.0.1] - 2026-01-28

Remove install metadata; add python3 to required binaries

## [1.0.0] - 2026-01-28

Initial release: Local speech-to-text using faster-whisper with 4-6x speed boost over OpenAI Whisper. GPU acceleration enables ~20x realtime transcription. Auto-setup scripts for Windows/Linux/macOS with CUDA detection. Supports distilled models for 6x additional speedup.
